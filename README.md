# Fastapi-Streamingresponse-Demo

FastAPI StreamingResponse demo

I have had do some digging to properly visualize LLM generated text in a ChatGPT-like UI. Here's a minimal example how it could work.

## Installation

Install dependencies using poetry

```bash
cd app
poetry install
./start-wrapper.sh
```

## Running locally

```bash
./start-wrapper.sh
# or
fastapi-streamingresponse-demo start-service --port 8000
```

## Development

**Add instructions for the future devs here**

Run pre-commit (if not hooked) before raising a merge request.
```bash
pre-commit run --all-files
```

Every once in a while update the pre-commit config
Run pre-commit before raising a merge request
```bash
pre-commit autoupdate
```
